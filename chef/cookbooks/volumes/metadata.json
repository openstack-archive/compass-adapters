{
  "name": "volumes",
  "description": "Mounts volumes as directed by node metadata. Can attach external cloud drives, such as ebs volumes.",
  "long_description": "# volumes chef cookbook\n\nMounts volumes as directed by node metadata. Can attach external cloud drives, such as ebs volumes.\n\n## Overview\n\nThis is a set of simple helpers for assigning components their locations on disk according to these common use cases:\n\n* standard directories: configuration, logs, lib files, etc. These are all the same, or should be.\n* directory is not a standard pattern, but follows conventions that let us configure it from node metadata.\n* directory prefers to be on the fastest-available drive, or a dedicated drive, or one that is persisted over the network.\n\n### standard_dir\n\nMost directories are **standard and boring**: `conf_dir`s go in `/etc/foo` and are `root:root 755`; `log_dir`s go in `/var/log/foo` and are `{user}:{group} 755`, and so forth. These DRY right up using the `standard_dirs` helper:\n\n        standard_dirs('lolcat.generator') do\n          directories   [:conf_dir, :log_dir, :pid_dir]\n        end\n\n  This doesn't just save keystrokes, it saves pager calls: the recipe is simpler to read (and thus maintain); it ensures the node metadata completely documents the state of the machine; and it wards off common pitfalls like \"configuration dirs owned by the daemon user\".\n\n  Standard directories don't have to be _completely_ devoid of individual character:\n\n        standard_dirs('lolcat.generator') do\n          directories   [:conf_dir, :html_cache_dir, :img_cache_dir, :log_dir, :pid_dir]\n        end\n\n  Both the `html_cache` and `rendered_cache` directories will follow cache directory conventions.\n\n### extra_dir\n\nIf you can't be boring, you should at least be **tastefully decorated**. Suppose your lolcat cookbook needs a 'caturday' directory (owned by the lolcat process, mode 0770) and a 'bukkit' directory (permissions `root:root 755`):\n\n        extra_dir(lolcat.generator.caturday_dir') do\n          user          :user\n          group         :group\n        end\n\n        extra_dir(lolcat.generator.bukkit_dir')\n\n  The `extra_dir` helper pulls its settings from the conventional node metadata (`node[:lolcat][:user]` `node[:lolcat][:generator][:caturday_dir]` and so forth), and falls back to conservative defaults.\n\n### volume_dirs\n\nLastly, some directory assignments -- typically the ones that relate to the machine's core purpose -- are **opinionated guests**.\n\nWhen my grandmother comes to visit, she quite reasonably asks for a room with a comfortable bed and a short climb. At my apartment, she stays in the main bedroom and I use the couch. At my brother's house, she enjoys the downstairs guest room.  If Ggrandmom instead demanded 'the master bedroom on the first floor', she'd find herself in the parking garage at my apartment, and uninvited from returning to visit my brother's house.\n\nSimilarly, the well-mannered cookbook does not hard-code a large data directory onto the root partition. Typically that's the private domain of the operating system, and there's a large and comfortably-appointed volume just for it to use. On the other hand, declaring a location of `/mnt/external2` will end in tears if I'm testing the cookbook on my laptop, where no such drive exists.\n\nThe solution is to request for volumes by their characteristics, and defer to the node's best effort in meeting that request.\n\n\n        # Data striped across all persistent dirs\n        volume_dirs('foo.datanode.data') do\n          type          :persistent, :bulk, :fallback\n          selects       :all\n          mode          \"0700\"\n        end\n\n        # Scratch space for indexing, striped across all scratch dirs\n        volume_dirs('foo.indexer.scratch') do\n          type          :local, :bulk, :fallback\n          selects       :all\n          mode          \"0755\"\n        end\n\n\nThese are commonly-used volume characteristic tags:\n\n* **fast**:       the 'fastest' volume available: on one machine this might be a dedicated SD drive or even a RAM drive; on another it might be the hey-its-the-only-drive-I-got drive.\n* **bulk**:       large storage area, preferably one that does not compete with the OS for space or access.\n* **local**:      low-latency / direct access.\n* **persistent**: storage that survives independently of its host machine\n* **fallback**:   states it's safe to use a general-purpose volume if no better match is present.\n\nAll of the above are positive rules: a volume is only `:fast` if it is labeled `:fast`. They are also passive rules: the cookbook makes no attempt to decide that say flash drives are `:fast` (it might be the SD card from my camera) or that a large drive is `:bulk` (it might be full, or read-only).\n\nThe `fallback` tag has additional rules:\n* if any volumes are tagged `fallback`, return the full set of `fallback`s;\n* otherwise, raise an error.\n\n#### Examples:\n\n* Web server: in production, database lives on one volume, logs are written to another. On a cheaper test server, just\n  put them whereever.\n\n* Isolate different apps, each on their own volume\n\n* Hadoop has the following mountable volume concerns:\n\n  - Namenode metadata -- *must* be persistent. Physical clusters typically mirror to one NFS and two local volumes.\n  - Datanode blocks   -- typically persistent. In a cloud environment, one strategy would be:\n    - where available, permanent attachable drives (EBS volumes)\n    - where available, local volumes (ephemeral drives)\n    - as a last resort, whatever's present.\n  - Scratch space for jobs -- should be fast, no need for it to be persistent.  On an EC2 instance, ephemeral drives\n    would be preferred.\n\n* Similarly, a Cassandra installation will place the commitlog the fastest available volume, the data store on the most\n  persistent available volume. A Mongo or MySQL admin may allocate high-demand tables on an SSD, the rest on normal disks.\n\nYou ask for volume_dirs with\n* a system\n* a component (optional)\n* a tag\n\nWe will look as follows:\n\n* volumes tagged 'foo-\n* volumes tagged 'foo-scratch'\n* volumes tagged 'foo'\n* volumes tagged 'scratch'\n\nWrite your recipes to request volumes\n\n\nNot doing this:\n\n        standard_dirs('lolcat.generator') do\n          conf_dir\n          log_dir       :mode => '0775'\n          pid_dir\n          cache_dir     :for => :img\n          cache_dir     :for => :html\n        end\n\n\n\n### assigning labels\n\nLabels are assigned by a human using (we hope) good taste -- there's no effort,\nnor will there be, to presuppose that flash drives are `fast` or large drives\nare `bulk`.  However, the cluster_chef provisioning tools do lend a couple\nhelpers:\n\n* `cloud(:ec2).defaults` describes a `:root`\n  - tags it as `fallback`\n  - if it is ebs, tags it\n  - does *not* marks it as `mountable`\n\n* `cloud(:ec2).mount_ephemerals` knows (from the instance type) what ephemeral\n  drives will be present. It:\n  - populates volumes `ephemeral0` through (up to) `ephemeral3`\n  - marks them as `mountable`\n  - tags them as `local`, `bulk` and `fallback`\n  - *removes* the `fallback` tag from the `:root` volume. (So be sure to call it *after*\n    calling `defaults`.\n\nYou can explicitly override any of the above.\n\n\n### examples\n\n\n* Hadoop namenode metadata:\n  - `:hadoop_namenode`\n  - `:hadoop`\n  - `[:persistent, :bulk]`\n  - `:bulk`\n  - `:fallback`\n\n\n\n    System       \tComponent      \tType\tPath           \tOwner         \tMode \tIndex \tattrs                          \tDescription\n    ------       \t---------      \t----\t----           \t-----         \t---- \t----- \t-----                          \t-----------\n\ntopline\n\n    hadoop      \tdfs_name       \tperm\thdfs/name      \thdfs:hadoop  \t0700\tall\t[:hadoop][:namenode   ][:data_dirs]\n    hadoop      \tdfs_2nn        \tperm\thdfs/secondary \thdfs:hadoop  \t0700\tall\t[:hadoop][:secondarynn][:data_dirs]    \tdfs.name.dir\n    hadoop      \tdfs_data       \tperm\thdfs/data      \thdfs:hadoop  \t0755\tall\t[:hadoop][:datanode   ][:data_dirs]    \tdfs.data.dir\n    hadoop      \tmapred_local   \tscratch\tmapred/local   \tmapred:hadoop\t0775\tall\t[:hadoop][:tasktracker][:scratch_dirs] \tmapred.local.dir\n    hadoop      \tlog      \tscratch\tlog      \thdfs:hadoop\t0775\tfirst\t[:hadoop][:log_dir]                \tmapred.local.dir\n    hadoop      \ttmp      \tscratch\ttmp      \thdfs:hadoop\t0777\tfirst\t[:hadoop][:tmp_dir]              \tmapred.local.dir\n\n    hbase       \tzk_data  \tperm\tzk/data  \thbase    \t0755\tfirst\t[:hbase][:zk_data_dir]  \t.\n    hbase          \ttmp      \tscratch\ttmp      \thbase    \t0755\tfirst\t[:hbase][:tmp_dir]       \t.\n\n    zookeeper       \tdata     \tperm\tdata     \tzookeeper\t0755\tfirst\t[:zookeeper][:data_dir]     \t.\n    zookeeper       \tjournal  \tperm\tjournal  \tzookeeper\t0755\tfirst\t[:zookeeper][:journal_dir]  \t.\n\n    elasticsearch \tdata    \tperm\tdata      \telasticsearch  \t0755\tfirst\t[:elasticsearch][:data_root]\t.\n    elasticsearch \twork    \tscratch\twork      \telasticsearch  \t0755\tfirst\t[:elasticsearch][:work_root]  \t.\n\n    cassandra       \tdata    \tperm\tdata     \tcassandra   \t0755\tall\t[:cassandra][:data_dirs]\n    cassandra         \tcommitlog     \tscratch\tcommitlog\tcassandra   \t0755\tfirst\t[:cassandra][:commitlog_dir]\n    cassandra         \tsaved_caches   \tscratch\tsaved_caches\tcassandra   \t0755\tfirst\t[:cassandra][:saved_caches_dir]\n\n    flume       \tconf    \t.\n    flume       \tpid     \t.\n    flume        \tdata     \tperm\tdata         \tflume\n    flume        \tlog      \tscratch\tdata       \tflume\n\n    zabbix\n    rundeck\n\n    nginx\n    mongodb\n\n    scrapers      \tdata_dir\n    api_stack    \t.\n    web_stack\n\nhold\n\n    redis       \tdata_dir\n    redis          \twork_dir\n    redis        \tlog_dir\n\n    statsd      \tdata_dir\n    statsd      \tlog _dir\n\n    graphite          \twhisper  \tperm\n    graphite          \tcarbon   \tperm\n    graphite          \tlog_dir  \tperm\n\n    mysql\n    sftp\n    varnish\n    ufw\n\nkill\n\n    tokyotyrant\n    openldap\n    nagios\n    apache2\n    rsyslog\n\n### Memoized\n\nBesides creating the directory, we store the calculated path into\n\n  node[:system][:component][:handle]\n\n## Recipes \n\n* `build_raid`               - Build a raid array of volumes as directed by node[:volumes]\n* `default`                  - Placeholder -- see other recipes in ec2 cookbook\n* `format`                   - Format the volumes listed in node[:volumes]\n* `mount`                    - Mount the volumes listed in node[:volumes]\n* `resize`                   - Resize mountables in node[:volumes] to fill the volume\n\n## Integration\n\nSupports platforms: debian and ubuntu\n\nCookbook dependencies:\n* metachef\n* xfs\n\n\n## Attributes\n\n* `[:volumes]`                        - Logical description of volumes on this machine (default: \"{}\")\n  - This hash maps an arbitrary name for a volume to its device path, mount point, filesystem type, and so forth.\n    \n    volumes understands the same arguments at the `mount` resource (nb. the prefix on `options`, `dump` and `pass`):\n    \n    * mount_point    (required to mount drive) The directory/path where the device should be mounted, eg '/data/redis'\n    * device         (required to mount drive) The special block device or remote node, a label or an uuid to mount, eg '/dev/sdb'. See note below about Xen device name translation.\n    * device_type    The type of the device specified -- :device, :label :uuid (default: `:device`)\n    * fstype         The filesystem type (`xfs`, `ext3`, etc). If you omit the fstype, volumes will try to guess it from the device.\n    * mount_options  Array or string containing mount options (default: `\"defaults\"`)\n    * mount_dump     For entry in fstab file: dump frequency in days (default: `0`)\n    * mount_pass     For entry in fstab file: Pass number for fsck (default: `2`)\n    \n    \n    volumes offers special helpers if you supply these additional attributes:\n    \n    * :scratch       if true, included in `scratch_volumes` (default: `nil`)\n    * :persistent    if true, included in `persistent_volumes` (default: `nil`)\n    * :attachable    used by the `ec2::attach_volumes` cookbook.\n    \n    Here is an example, typical of an amazon m1.large machine:\n    \n      node[:volumes] = { :volumes => {\n          :scratch1 => { :device => \"/dev/sdb\",  :mount_point => \"/mnt\", :scratch => true, },\n          :scratch2 => { :device => \"/dev/sdc\",  :mount_point => \"/mnt2\", :scratch => true, },\n          :hdfs1    => { :device => \"/dev/sdj\",  :mount_point => \"/data/hdfs1\", :persistent => true, :attachable => :ebs },\n          :hdfs2    => { :device => \"/dev/sdk\",  :mount_point => \"/data/hdfs2\", :persistent => true, :attachable => :ebs },\n        }\n      }\n    \n    It describes two scratch drives (fast local storage, but wiped when the machine is torn down) and two persistent drives (network-attached virtual storage, permanently available).\n    \n    Note: On Xen virtualization systems (eg EC2), the volumes are *renamed* from /dev/sdj to /dev/xvdj -- but the amazon API requires you refer to it as /dev/sdj.\n    \n    If the `node[:virtualization][:system]` is 'xen' **and** there are no /dev/sdXX devices at all **and** there are /dev/xvdXX devices present, volumes will internally convert any device point of the form `/dev/sdXX` to `/dev/xvdXX`. If the example above is a Xen box, the values for :device will instead be `\"/dev/xvdb\"`, `\"/dev/xvdc\"`, `\"/dev/xvdj\"` and `\"/dev/xvdk\"`.\n    \n* `[:metachef][:aws_credential_source]` -  (default: \"data_bag\")\n  - where should we get the AWS keys?\n* `[:metachef][:aws_credential_handle]` -  (default: \"main\")\n  - the key within that data bag\n\n## License and Author\n\nAuthor::                Philip (flip) Kromer - Infochimps, Inc (<coders@infochimps.com>)\nCopyright::             2011, Philip (flip) Kromer - Infochimps, Inc\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n\n> readme generated by [cluster_chef](http://github.com/infochimps/cluster_chef)'s cookbook_munger\n",
  "maintainer": "Philip (flip) Kromer - Infochimps, Inc",
  "maintainer_email": "coders@infochimps.com",
  "license": "Apache 2.0",
  "platforms": {
    "debian": ">= 0.0.0",
    "ubuntu": ">= 0.0.0"
  },
  "dependencies": {
    "metachef": ">= 0.0.0",
    "xfs": ">= 0.0.0"
  },
  "recommendations": {
  },
  "suggestions": {
  },
  "conflicting": {
  },
  "providing": {
  },
  "replacing": {
  },
  "attributes": {
    "volumes": {
      "display_name": "Logical description of volumes on this machine",
      "description": "This hash maps an arbitrary name for a volume to its device path, mount point, filesystem type, and so forth.\n\nvolumes understands the same arguments at the `mount` resource (nb. the prefix on `options`, `dump` and `pass`):\n\n* mount_point    (required to mount drive) The directory/path where the device should be mounted, eg '/data/redis'\n* device         (required to mount drive) The special block device or remote node, a label or an uuid to mount, eg '/dev/sdb'. See note below about Xen device name translation.\n* device_type    The type of the device specified -- :device, :label :uuid (default: `:device`)\n* fstype         The filesystem type (`xfs`, `ext3`, etc). If you omit the fstype, volumes will try to guess it from the device.\n* mount_options  Array or string containing mount options (default: `\"defaults\"`)\n* mount_dump     For entry in fstab file: dump frequency in days (default: `0`)\n* mount_pass     For entry in fstab file: Pass number for fsck (default: `2`)\n\n\nvolumes offers special helpers if you supply these additional attributes:\n\n* :scratch       if true, included in `scratch_volumes` (default: `nil`)\n* :persistent    if true, included in `persistent_volumes` (default: `nil`)\n* :attachable    used by the `ec2::attach_volumes` cookbook.\n\nHere is an example, typical of an amazon m1.large machine:\n\n  node[:volumes] = { :volumes => {\n      :scratch1 => { :device => \"/dev/sdb\",  :mount_point => \"/mnt\", :scratch => true, },\n      :scratch2 => { :device => \"/dev/sdc\",  :mount_point => \"/mnt2\", :scratch => true, },\n      :hdfs1    => { :device => \"/dev/sdj\",  :mount_point => \"/data/hdfs1\", :persistent => true, :attachable => :ebs },\n      :hdfs2    => { :device => \"/dev/sdk\",  :mount_point => \"/data/hdfs2\", :persistent => true, :attachable => :ebs },\n    }\n  }\n\nIt describes two scratch drives (fast local storage, but wiped when the machine is torn down) and two persistent drives (network-attached virtual storage, permanently available).\n\nNote: On Xen virtualization systems (eg EC2), the volumes are *renamed* from /dev/sdj to /dev/xvdj -- but the amazon API requires you refer to it as /dev/sdj.\n\nIf the `node[:virtualization][:system]` is 'xen' **and** there are no /dev/sdXX devices at all **and** there are /dev/xvdXX devices present, volumes will internally convert any device point of the form `/dev/sdXX` to `/dev/xvdXX`. If the example above is a Xen box, the values for :device will instead be `\"/dev/xvdb\"`, `\"/dev/xvdc\"`, `\"/dev/xvdj\"` and `\"/dev/xvdk\"`.\n",
      "default": "{}",
      "choice": [

      ],
      "calculated": false,
      "type": "string",
      "required": "optional",
      "recipes": [

      ]
    },
    "metachef/aws_credential_source": {
      "display_name": "",
      "description": "where should we get the AWS keys?",
      "default": "data_bag",
      "choice": [

      ],
      "calculated": false,
      "type": "string",
      "required": "optional",
      "recipes": [

      ]
    },
    "metachef/aws_credential_handle": {
      "display_name": "",
      "description": "the key within that data bag",
      "default": "main",
      "choice": [

      ],
      "calculated": false,
      "type": "string",
      "required": "optional",
      "recipes": [

      ]
    }
  },
  "groupings": {
  },
  "recipes": {
    "volumes::default": "Placeholder -- see other recipes in ec2 cookbook",
    "volumes::mount": "Mount the volumes listed in node[:volumes]",
    "volumes::build_raid": "Build a raid array of volumes as directed by node[:volumes]",
    "volumes::resize": "Resize mountables in node[:volumes] to fill the volume",
    "volumes::format": "Format the volumes listed in node[:volumes]"
  },
  "version": "3.0.4"
}